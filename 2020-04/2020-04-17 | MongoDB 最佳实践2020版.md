- [原文链接](https://www.percona.com/blog/2020/04/17/mongodb-best-practices-2020-edition/)

# MongoDB 最佳实践2020版
- 在这篇博客中，我们将讨论在操作系统（OS）和MongoDB层构建MongoDB生态系统的最佳实践。这篇文章的主要目的是分享我过去几年调优MongoDB的经验，并对我遇到的各种问题的一个总结。
- **剧透警告**：这篇文章关注的是**MongoDB3.6及其以上版本**，因为以前的版本已达生命周期。
- 注意，调优设置的目的不仅是为了提高性能，而且还增强了MongoDB的高可用性和弹性。
- 言归正传，让我们从操作系统设置开始。
## 操作系统设置
### Swappiness
- **Swappiness**是一个Linux内核设置，它影响虚拟内存管理器是否需要分配swap，可以设为0-100。设置为“0”表示告诉内核使用swap仅避免OOM的问题（注：设置为0，Linux3.5以及以上:宁愿OOM也不用swap，Linux3.4以及更早：宁愿用swap也不OOM）。设定为100，操作系统会主动的使用swap，交换到磁盘。Linux的默认值通常是60，这对数据库的使用并不理想。
- 通常对数据库服务器，设置为“0”（或者有时设置为“10”），这告诉内核为了获得更好的响应时间而选择内存。然而，Ovais Tariq详细描述了一个设置为“0”时的已知的[bug](https://www.percona.com/blog/2014/04/28/oom-relation-vm-swappiness0-new-kernel/)（或特性）。
- 所以推荐设置为“1”。更改`swappiness`的值：
```
#没有持久化 - 如果您重启操作系统，这个值会还原成原来的值
echo 1 > /proc/sys/vm/swappiness
```
- 重启并持久化
```
# 在`sysctl.conf`中更改，重启后更改持久化
sudo sysctl -w vm.swappiness=1
```
### NUMA架构
- NUMA是一种内存设计，其中对称多处理系统处理器（SMP）可以比非本地内存（分配给其他CPU的本地内存）更快地访问其本地内存。下面是一个启用了NUMA的系统示例：
```
# NUMA system
$ numactl --hardware
 
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 24 25 26 27 28 29 30 31 32 33 34 35
node 0 size: 130669 MB
node 0 free: 828 MB
node 1 cpus: 12 13 14 15 16 17 18 19 20 21 22 23 36 37 38 39 40 41 42 43 44 45 46 47
node 1 size: 131072 MB
node 1 free: 60 MB
node distances:
node   0   1 
  0:  10  21 
  1:  21  10
```
- 我们可以看到，node0比node1有更多的可用内存。这里有一个问题，导致操作系统即使有可用内存也会swap。swap问题在Jeremy Cole的[“the Swap Insanity and NUMA Architecture”](https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/)博文中进行了解释。这篇博文侧重于MySQL，但它对MongoDB同样适用。
- 不幸的是，MongoDB不支持NUMA，因此，MongoDB可能会内存分配不均，甚至在内存可用的情况下也会使用swap。为了解决这个问题，`mongod`进程可以通过两种方式使用交错模式（所有节点上公平分配内存）：
	- 启动mongod进程时，指定`numactl --interleave=all`:
	```
	numactl --interleave=all /usr/bin/mongod -f /etc/mongod.conf
	```
	- 或者使用**systemd**
	```
	# Edit the file
	/etc/systemd/system/multi-user.target.wants/mongod.service
	```
   如果`ExecStart`如下所示：
	```
	ExecStart=/usr/bin/mongod --config /etc/mongod.conf
	```
   将其更改为：
    
	```
	ExecStart=/usr/bin/numactl --interleave=all /usr/bin/mongod --config /etc/mongod.conf
	```
  
   将更改应用`systemd`：
    
  ```
	sudo systemctl daemon-reload
	```
    
   重启`mongod`实例：
    
	```
	sudo systemctl stop mongod
  sudo systemctl start mongod
	```
    
  并检查内存使用情况：
    
	```
		$ sudo numastat -p $(pidof mongod)
 
		Per-node process memory usage (in MBs) for PID 35172 (mongod)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
		Huge                         0.00            0.00            0.00
		Heap                        19.40           27.36           46.77
		Stack                        0.03            0.03            0.05
		Private                      1.61           24.23           25.84
		----------------  --------------- --------------- ---------------
		Total                       21.04           51.62           72.66

	```
### `zone_reclaim_mode`
- 在一些操作系统版本中，启用了`vm.zone_reclaim_mode`。`zone_reclaim_mode`允许用户在一个区域用尽内存时设置或多或少积极的方法来回收内存。如果将其设置为0，则不会发生区域回收。
- 在NUMA启用时，有必要禁用`vm.zone_reclaim_mode`。要禁用，您可以执行以下命令：
```
sudo sysctl -w vm.zone_reclaim_mode=0
```
### IO调度
- `IO scheduler`是内核用于提交读写磁盘的算法。默认情况下，大多数Linux使用`CFQ`(完全公平队列)调度器。CFQ在许多通用情况下都能很好地工作，但是几乎没有延时保证。另外两个调度器是`deadline`和`noop`。`deadline`在延时敏感的用例（如数据库）方面表现突出，`noop`几乎不调度。对于裸盘，`deadline`和`noop`之间的任意算法（两者之间的性能差异是难以察觉的）都优于`CFQ`。
- 如果您在VM中运行MongoDB（VM有自身IO调度器），最好使用`noop`，让虚拟化层自己处理IO调度。
- 用root用户更改（相应的磁盘）
```
#验证
$ cat /sys/block/xvda/queue/scheduler
noop [deadline] cfq

#动态调整
$ echo "noop" > /sys/block/xvda/queue/scheduler
```
- 要使更改永久生效，您必须编辑GRUB配置文件（通常是`/etc/sysconfig/grub`），并在`GRUB_CMDLINE_LINUX_DEFAULT`添加一个`elevator`选项。例如，您可以将这行：
```
GRUB_CMDLINE_LINUX="console=tty0 crashkernel=auto console=ttyS0,115200
```	
- 更改为
```
GRUB_CMDLINE_LINUX="console=tty0 crashkernel=auto console=ttyS0,115200 elevator=noop"	
```
- **注意AWS设置**：在某些情况下，I/O调度器的值为`none`，尤其是在AWS VM实例类型中，其中EBS卷为NVMe块设备。这是因为该设置在PCIe/NVMe设备中没有使用。原因是它们内部有大量的内部队列，并且完全绕过了IO调度器。在这个场景下，设置为`none`，这在这类磁盘中是最优的。

### 透明大页
- 数据库使用小的内存页，而透明大页往往变得碎片化，影响性能：
- 在RHEL/CENTOS 6和7中禁用
```
$ echo "never" > /sys/kernel/mm/transparent_hugepage/enabled
$ echo "never" > /sys/kernel/mm/transparent_hugepage/defrag
```
- 要使这个更改在服务器重启后仍然有效，您需要在内核配置（`/etc/sysconfig/grub`）中添加`transparent_hugepage=never`
```
GRUB_CMDLINE_LINUX="console=tty0 crashkernel=auto console=ttyS0,115200 elevator=noop transparent_hugepage=never"
```
- 通过执行`grub2-mkconfig -o`命令重建`/boot/grub2/grub.cfg`文件。在重建GRUB2配置文件前，确认您已经对现有`/boot/grub2/grub.cfg`做了备份。

#### 基于BIOS的机器
```
$ grub2-mkconfig -o /boot/grub2/grub.cfg
```
#### 故障定位
- 如果THP仍未关闭，继续使用以下选项：

#### 禁用调试服务
- 使用以下命令禁用会重新启用THP的调优服务
```
$ systemctl stop tuned
$ systemctl disable tuned
```

### 脏页刷新比
- `dirty_ratio`是系统内存能够容纳脏页的百分比。大多数Linux主机的默认值在20%-30%之间。当超过限制，脏页将落盘，从而有一个小暂停。为了避免暂停，有一个第二比率`dirty_background_ratio`（默认为10-15%），它告诉内核后台刷新脏页落盘，不需要任何暂停。
- 对“dirty_ratio”来说，20-30%是一个很好的默认值，但是在大内存数据库服务器上，这可能会占用大量内存。例如，在128G的主机上，最多可以允许38.4G的脏页。后台进程不会在12.8G之前触发。建议降低此设置并监控对查询性能和磁盘IO的影响。目标是在不影响查询性能的情况下减少内存使用。
- 对于大内存数据库服务器（64GB），推荐的设置是：`vm.dirty_ratio = 15`&`vm.dirty_background_ratio = 5`，或者更少。（对于高性能/大内存服务器，红帽建议设置为10和3）。
- 您可以通过设置`/etc/sysctl.conf`
```
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
```
### 文件系统挂载选项
- MongoDB推荐使用`XFS`文件系统来存储磁盘上的数据库数据。此外，适当的挂载选项可以显著提升性能。确保挂载选项中有`noatime`，有RAID卡和BBU。使用以下选项重新挂载`/mnt/db/`
```
mount -oremount,rw,noatime/mnt/db
```
- 有必要在`/etc/fstab`添加编辑相应的行，以便在重新启动后持久化这些选项。如：
```
UUID=f41e390f-835b-4223-a9bb-9b45984ddf8d /                       xfs     rw,noatime,attr2,inode64,noquota        0 0
```

### Unix的ulimit设置
- 大多数类UNIX操作系统，包括Linux和macOS，都提供了基于每个进程和每个用户来限制和控制系统资源（如线程、文件和网络链接）使用的方法。这些“ulimit”防止单个用户使用太多的系统资源。有时，这些限制的默认值很低，可能会在常规的MongoDB操作过程中引起多个问题。对使用`systemd`的Linux，您可以在`[service]`中指定限制：
- 首先，打开文件并编辑：
```
vi /etc/systemd/system/multi-user.target.wants/mongod.service
```
- 在`[Service]`中添加以下部分
```
# (file size) 
LimitFSIZE=infinity 
# (cpu time) LimitCPU=infinity 
# (virtual memory size) 
LimitAS=infinity 
# (locked-in-memory size) 
LimitMEMLOCK=infinity 
# (open files) 
LimitNOFILE=64000 
# (processes/threads) 
LimitNPROC=64000
```
- 对为用户调整：
```
# Edit the file below
/etc/security/limits.conf
```
- 添加启动`mongod`进程的用户
```
# In this example, the user is mongo
mongo hard cpu  unlimited
mongo soft cpu  unlimited
mongo hard memlock unlimited
mongo soft memlock unlimited
mongo hard nofile 64000
mongo soft nofile 64000
mongo hard nproc 192276
mongo soft nproc 192276
mongo hard fsize unlimited
mongo soft fsize unlimited
mongo hard as unlimited
mongo soft as unlimited
```
- 为了提高性能，我们可以将超级用户root的进程限制设置为无限制。编辑`.bashrc`文件，添加以下内容：
```
# vi /root/.bashrc
ulimit -u unlimited
```
- 退出并重新登陆终端以使更改生效。

### 网络堆栈
- Linux内核网络调优的几个默认值对MongoDB而言不是最佳选择，对典型的千兆网卡主机（或者万兆）的限制可能导致路由器和负载均衡器不可预知的行为。我建议增加相对较低的吞吐量设置（`net.core.somaxconn`和`net.ipv4.tcp_max_syn_backlog`）并减少keepalive相关参数设置，如下所示：
- 将以下内容添加到`/etc/sysctl.conf`文件中（如果`/etc/sysctl.d`存在添加一个新文件`/etc/sysctl.d/mongodb-sysctl.conf`）:
```
net.core.somaxconn = 4096
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_time = 120
net.ipv4.tcp_max_syn_backlog = 4096
net.ipv4.tcp_keepalive_probes = 6
```
- 注意：您必须用root用户或者sudo执行`/sbin/sysctl -p`命令来应用这些更改（或者重启）。

### NTP服务
- 所有这些更深层次的调优使您很容易忘记一些简单的事情，比如您的时钟源。由于MongoDB是一个集群，它依赖于节点之间一致时间。因此，NTP守护进程应该在包括mongos和仲裁节点在内的所有MongoDB主机上永久运行。
- 在RedHat/CentOS上安装
```
sudo yum install ntp
```

## MongoDB设置

### Journal提交间隔
- 可以设为1到500毫秒（默认为200毫秒）。较低的值保证了日志的持久化，但会降低磁盘性能。由于MongoDB通常是一个复制集，可以调大这个参数来获得更好的性能：
```
# edit /etc/mongod.conf
storage:
  journal:
    enabled: true
    commitIntervalMs: 300
```
### WiredTiger cache
- 对于专用服务器，可以增加WiredTiger（WT）缓存。默认情况下，它使用内存的50%+1GB。将这个值设置为内存的60%-70%并监控内存使用情况。例如将WT cache设置为50G：
```
# edit /etc/mongod.conf
wiredTiger:
   engineConfig:
      cacheSizeGB: 50
```
- 如果有适当的监控工具，比如PMM，就可以监控内存使用情况。例如：
![enter image description here](https://www.percona.com/blog/wp-content/uploads/2020/03/Screen-Shot-2020-03-24-at-21.38.10.png)
### Read/Write tickets
- WiredTiger使用tickets来控制存储引擎同时处理读/写操作的数量。默认值是128，大多数情况都适用，但是在某些情况下，tickets数量是不够的。调整如下：
```
use admin
db.adminCommand( { setParameter: 1, wiredTigerConcurrentReadTransactions: 256 } )
db.adminCommand( { setParameter: 1, wiredTigerConcurrentWriteTransactions: 256 } )
```
- 持久化需要添加到Mongo配置文件：
```
# Two options below can be used for wiredTiger and inMemory storage engines
setParameter:
    wiredTigerConcurrentReadTransactions: 256
    wiredTigerConcurrentWriteTransactions: 256
```
- 为了观察负载行为，进行评估是必要的。同样，PMM也适用与这种情况：
![enter image description here](https://www.percona.com/blog/wp-content/uploads/2020/03/Screen-Shot-2020-03-24-at-21.44.50.png)

- 注意，有时增加并行度可能会导致与已过载服务器所期望相反的结果。此时，可能需要将tickets数量减少到当前可用cpu/vcpu数量（如果服务器有16核，则为每个设置16个读/写tickets）。**这个参数需要进行广泛的测试！**

### 容器中`mongos`的陷阱
- `mongos`进程不支持`cgroups`，这意味着它会在创建大量TaskExecutor线程时消耗大量CPU资源。其次，在K8S的Pods中分组容器最终会创建大量的`mongos`进程，从而导致额外的开销。自动化有时也会遇到这个问题（例如使用ansible）。一般来说，DevOps工程师倾向于创建一个`mongos`资源池。
- 为了避免池子过大，在运行容器化的`mongos`时使用以下参数指定`taskExecutorPoolSize`，或者在配置文件中设置这个参数：`--setParameter taskExecutorPoolSize=X`，X是分配给容器的CPU核数（例如，K8S中的'CPU limits'或者Docker中的'cpuset/cpus'）。例如：
```
$ /opt/mongodb/4.0.6/bin/mongos --logpath /home/vinicius.grippa/data/data/mongos.log --port 37017 --configdb configRepl/localhost:37027,localhost:37028,localhost:37029 --keyFile /home/vinicius.grippa/data/data/keyfile --fork --setParameter taskExecutorPoolSize=1
```
- 或使用配置文件
```
setParameter:
     taskExecutorPoolSize: 1
```
## 总结
- 我试图涵盖和总结我在日常中看到的最常见的问题和不正确的设置。使用推荐设置并不是万能的，但它涵盖大多数情况，并帮助那些使用MongoDB的用户获得更好的用户体验。最后，合理的监控系统是必须的，根据应用负载调整设置。
